{
    "contents" : "library(reshape2)\nlibrary(dplyr)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(RMySQL)\n\n\n\nsearch_terms = c('transferwise'\n                 ,'azimo'\n                 ,'worldfirst'\n                 ,'worldremit'\n                 ,'transfergo'\n                 ,'tipalti'\n                 ,'travelex'\n                 ,'weswap'\n                 ,'xendpay'\n                 ,'hyperwallet'\n                 ,'paypal'\n                 ,'currency fair'\n                 ,'revolut'\n                 ,'western union'\n                 ,'ofx'\n                 ,'xoom'\n                 ,'payoneer'\n                 )\n\n\nURL_GT=function(keyword=\"\", country=NA, region=NA, year=NA, month=1, length=3){\n  \n  start=\"http://www.google.com/trends/trendsReport?hl=en-US&q=\"\n  end=\"&cmpt=q&content=1&export=1\"\n  geo=\"\"\n  date=\"\"\n  \n  #Geographic restrictions\n  if(!is.na(country)) {\n    geo=\"&geo=\"\n    geo=paste(geo, country, sep=\"\")\n    if(!is.na(region)) geo=paste(geo, \"-\", region, sep=\"\")\n  }\n  \n  queries=keyword[1]\n  if(length(keyword)>1) {\n    for(i in 2:length(keyword)){\n      queries=paste(queries, \"%2C \", keyword[i], sep=\"\")\n    }\n  }\n  \n  #Dates\n  if(!is.na(year)){\n    date=\"&date=\"\n    date=paste(date, month, \"%2F\", year, \"%20\", length, \"m\", sep=\"\")\n  }\n  \n  URL=paste(start, queries, geo, date, end, sep=\"\")\n  URL <- gsub(\" \", \"%20\", URL)\n  return(URL)\n}\n\ndownloadGT=function(URL, downloadDir){\n  \n  #Determine if download has been completed by comparing the number of files in the download directory to the starting number\n  startingFiles=list.files(downloadDir)\n  browseURL(URL)\n  endingFiles=list.files(downloadDir)\n  Sys.sleep(1)\n  while(length(setdiff(endingFiles,startingFiles))==0) {\n    Sys.sleep(3)\n    endingFiles=list.files(downloadDir)\n  }\n  filePath=setdiff(endingFiles,startingFiles)\n  return(filePath)\n}\n\n\nreadGT=function(filePath){\n  rawFiles=list()\n  \n  for(i in 1:length(filePath)){\n    if(length(filePath)==1) rawFiles[[1]]=read.csv(filePath, header=F, blank.lines.skip=F)\n    if(length(filePath)>1) rawFiles[[i]]=read.csv(filePath[i], header=F, blank.lines.skip=F)\n  }\n  \n  output=data.frame()\n  name=vector()\n  \n  for(i in 1:length(rawFiles)){\n    data=rawFiles[[i]]\n    name=as.character(t(data[5,-1]))\n    \n    #Select the time series\n    start=which(data[,1]==\"\")[1]+3\n    stop=which(data[,1]==\"\")[2]-2\n    \n    #Skip to next if file is empty\n    if(ncol(data)<2) next\n    if(is.na(which(data[,1]==\"\")[2]-2)) next\n    \n    data=data[start:stop,]\n    data[,1]=as.character(data[,1])\n    \n    #Convert all columns except date column into numeric\n    for(j in 2:ncol(data)) data[,j]=as.numeric(as.character(data[,j]))\n    \n    #FORMAT DATE\n    len=nchar(data[1,1])\n    \n    #Monthly data\n    if(len==7) {\n      data[,1]=as.Date(paste(data[,1], \"-1\", sep=\"\"), \"%Y-%m-%d\")\n      data[,1]=sapply(data[,1], seq, length=2, by=\"1 month\")[2,]-1\n      data[,1]=as.Date(data[,1], \"%Y-%m-%d\", origin=\"1970-01-01\")\n    }\n    \n    #Weekly data\n    if(len==23){\n      data[,1]=sapply(data[,1], substr, start=14, stop=30)\n      data[,1]=as.Date(data[,1], \"%Y-%m-%d\")\n    }\n    \n    #Daily data\n    if(len==10) data[,1]=as.Date(data[,1], \"%Y-%m-%d\")\n    \n    #Structure into panel data format\n    panelData=data[1:2]\n    panelData[3]=name[1]\n    names(panelData)=c(\"Date\", \"SVI\", \"Keyword\")\n    if(ncol(data)>2) {\n      \n      for(j in 3:ncol(data)) {\n        appendData=data[c(1,j)]\n        appendData[3]=name[j-1]\n        names(appendData)=c(\"Date\", \"SVI\", \"Keyword\")\n        panelData=rbind(panelData, appendData)\n      }\n    }\n    \n    #Add file name  \n    panelData[ncol(panelData)+1]=filePath[i]\n    \n    #Add path to filename\n    names(panelData)[4]=\"Path\"\n    \n    #Merge several several files into one\n    if(i==1) output=panelData\n    if(i>1) output=rbind(output, panelData)\n  }\n  return(output)\n}\nreadAdditionalGT = function(filePath){\n  output=list()\n  rawFiles=list()\n  for(i in 1:length(filePath)){\n    if(length(filePath)==1) rawFiles[[1]]=read.csv(filePath, header=F, blank.lines.skip=F)\n    if(length(filePath)>1) rawFiles[[i]]=read.csv(filePath[i], header=F, blank.lines.skip=F)\n  }\n  \n  \n  for(file in rawFiles){\n    search_term = substring(as.character(file[1,1]), 22)\n    start = grep('Top regions', file[,1]) + 2\n    if(length(start)>0){\n      end = start + which(file[start:nrow(file),1]==\"\")[1] - 2\n      tmp = file[start:end,]\n      tmp$Type = 'Top regions'\n      tmp$Keyword = search_term\n      output[[length(output)+1]] = tmp\n    }\n    \n    start = grep('Top cities', file[,1]) + 2\n    if(length(start)>0){\n      end = start + which(file[start:nrow(file),1]==\"\")[1] - 2\n      tmp = file[start:end,]\n      tmp$Type = 'Top cities'\n      tmp$Keyword = search_term\n      output[[length(output)+1]] = tmp\n    }\n    \n    start = grep('Top searches', file[,1]) + 2\n    if(length(start)>0){\n      end = start + which(file[start:nrow(file),1]==\"\")[1] - 2\n      tmp = file[start:end,]\n      tmp$Type = 'Top searches'\n      tmp$Keyword = search_term\n      output[[length(output)+1]] = tmp\n    }\n    \n    start = grep('Rising searches', file[,1]) + 2\n    if(length(start)>0){\n      end = start + which(file[start:nrow(file),1]==\"\")[1] - 2\n      tmp = file[start:end,]\n      tmp$Type = 'Rising searches'\n      tmp$Keyword = search_term\n      tmp$V2 = length(tmp$V2):1\n      output[[length(output)+1]] = tmp\n    }\n  }\n  output = do.call(rbind, output)\n  return(output)\n}\n\n\n\n\n# Get individual data\nurls_individual = sapply(search_terms, URL_GT)\ndownloadDir = '/Users/erik.johansson/downloads'\npaths_individual = list()\nfor(url in urls_individual){\n  paths_individual[length(paths_individual)+1] = downloadGT(url, downloadDir)\n}\n\npaths_individual = unlist(paths_individual)\npaths_individual = paste(downloadDir, paths_individual, sep='/')\nindividual_gt_data = readGT(paths_individual)\n#substring(paths_individual[13], 41,42)='44'\n#paths_individual = paste(downloadDir, '/', 'report (', 45:60, ').csv', sep='')\nadditional_data = readAdditionalGT(paths_individual)\nnames(additional_data) = c('Term', 'Value', 'Type', 'Keyword')\nadditional_data$Value = as.numeric(as.character(additional_data$Value))\nsystem('ssh -N -f stats@looker.transferwise.com -L 3307:172.18.1.43:3306')\n\ncon <- dbConnect(MySQL(),\n                 user = .env$usr,\n                 password = .env$pw,\n                 host = '127.0.0.1',\n                 port = 3307,\n                 dbname='obfuscated')\n\nsearch_terms_table = data.frame(keyword_id = c(1:length(search_terms)), Keyword = search_terms)\ndbWriteTable(con, 'tmp_google_trends_search_terms', search_terms_table, row.names=F, overwrite=T)\ndbSendQuery(con, 'alter table tmp_google_trends_search_terms add index (keyword_id)')\n\nadditional_data$Term = as.character(additional_data$Term)\nadditional_data$Term[which(additional_data$Term == 'United States')] = 'United States of America'\nadditional_data = merge(additional_data, search_terms_table, by = 'Keyword')\nadditional_data$Date_added = Sys.time()\ndbWriteTable(con, 'tmp_google_trends_additional_info', additional_data, row.names=F, overwrite=T)\ndbSendQuery(con, 'alter table tmp_google_trends_additional_info add index (keyword_id)')\n\nsystem('killall ssh')\n\n# Get comparison data\nurls = list()\nfor(term in search_terms[-1]){\n  urls[length(urls)+1] = URL_GT(c(term, search_terms[1]))\n}\nurls = unlist(urls)\n\npaths = list()\nfor(url in urls){\n  paths[length(paths)+1] = downloadGT(url, downloadDir)\n}\n\npaths = unlist(paths)\npaths = paste(downloadDir, paths, sep='/')\n\ncomparable_gt_data = lapply(paths, readGT)\ncomparable_gt_data = lapply(comparable_gt_data, function(x){dcast(x, Date~Keyword, value.var = 'SVI')})\n\nratio = list()\nfor(datum in comparable_gt_data){\n  colnames = names(datum)\n  numerator = which(colnames != search_terms[1] & colnames != 'Date')\n  denominator = which(colnames == search_terms[1])\n  res = datum[numerator]/datum[denominator]\n  ratio[[length(ratio)+1]] = res\n  ratio[[length(ratio)]]$Keyword = colnames[numerator]\n  ratio[[length(ratio)]]$Date = datum[,1]\n}\nratio = rbindlist(ratio)\nnames(ratio) = c('Ratio', 'Keyword', 'Date')\ngoogle_trends_data = merge(individual_gt_data, ratio, by=c('Date', 'Keyword'), all.x=T)\ngoogle_trends_data = google_trends_data[-4]\ngoogle_trends_data$Comparable_SVI = google_trends_data$Ratio * google_trends_data$SVI\nmain_search_term = which(google_trends_data$Keyword == search_terms[1])\ngoogle_trends_data$Comparable_SVI[main_search_term] = google_trends_data$SVI[main_search_term]\ngoogle_trends_data = merge(google_trends_data, search_terms_table, by = 'Keyword')\ngoogle_trends_data$Date_added = Sys.time()\n\nsystem('ssh -N -f stats@looker.transferwise.com -L 3307:172.18.1.43:3306')\n\ncon <- dbConnect(MySQL(),\n                 user = .env$usr,\n                 password = .env$pw,\n                 host = '127.0.0.1',\n                 port = 3307,\n                 dbname='obfuscated')\n\n\ndbWriteTable(con, 'tmp_google_trends', google_trends_data, row.names=F, overwrite=T)\ndbSendQuery(con, 'alter table tmp_google_trends add index (keyword_id)')\nsystem('killall ssh')\n",
    "created" : 1475156875527.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1044446941",
    "id" : "FE7118E1",
    "lastKnownWriteTime" : 1475156254,
    "path" : "~/transferwise competitor data.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "type" : "r_source"
}