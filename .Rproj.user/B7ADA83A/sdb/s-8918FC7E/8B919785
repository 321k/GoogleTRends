{
    "contents" : "# The Google Trends package avoids issues with authentication to Google Trends by downloading the files via the browser.\n# There are four key function in the GoogleTRends package. URL_GT, downloadGT, readGT and readAdditionalGT.\n\n# URL_GT generates the URL for downloading the CSV file from Google Trends.\nURL_GT=function(keyword=\"\", country=NA, region=NA, year=NA, month=1, length=3){\n\n  start=\"http://www.google.com/trends/trendsReport?hl=en-US&q=\"\n  end=\"&cmpt=q&content=1&export=1\"\n  geo=\"\"\n  date=\"\"\n\n  #Geographic restrictions\n  if(!is.na(country)) {\n    geo=\"&geo=\"\n    geo=paste(geo, country, sep=\"\")\n    if(!is.na(region)) geo=paste(geo, \"-\", region, sep=\"\")\n  }\n\n  queries=keyword[1]\n  if(length(keyword)>1) {\n    for(i in 2:length(keyword)){\n      queries=paste(queries, \"%2C \", keyword[i], sep=\"\")\n    }\n  }\n\n  #Dates\n  if(!is.na(year)){\n    date=\"&date=\"\n    date=paste(date, month, \"%2F\", year, \"%20\", length, \"m\", sep=\"\")\n  }\n\n  URL=paste(start, queries, geo, date, end, sep=\"\")\n  URL <- gsub(\" \", \"%20\", URL)\n  return(URL)\n}\n\n# downloadGT takes the URL from the URL_GT function, downloads the file and returns the file name.\ndownloadGT=function(URL, downloadDir){\n\n  #Determine if download has been completed by comparing the number of files in the download directory to the starting number\n  startingFiles=list.files(downloadDir)\n  browseURL(URL)\n  endingFiles=list.files(downloadDir)\n  Sys.sleep(1)\n  while(length(setdiff(endingFiles,startingFiles))==0) {\n    Sys.sleep(3)\n    endingFiles=list.files(downloadDir)\n  }\n  filePath=setdiff(endingFiles,startingFiles)\n  return(filePath)\n}\n\n# readGT takes the file name from the downlaodGT function and reads the time series data from the files.\nreadGT=function(filePath){\n  rawFiles=list()\n\n  for(i in 1:length(filePath)){\n    if(length(filePath)==1) rawFiles[[1]]=read.csv(filePath, header=F, blank.lines.skip=F)\n    if(length(filePath)>1) rawFiles[[i]]=read.csv(filePath[i], header=F, blank.lines.skip=F)\n  }\n\n  output=data.frame()\n  name=vector()\n\n  for(i in 1:length(rawFiles)){\n    data=rawFiles[[i]]\n    name=as.character(t(data[5,-1]))\n\n    #Select the time series\n    start=which(data[,1]==\"\")[1]+3\n    stop=which(data[,1]==\"\")[2]-2\n\n    #Skip to next if file is empty\n    if(ncol(data)<2) next\n    if(is.na(which(data[,1]==\"\")[2]-2)) next\n\n    data=data[start:stop,]\n    data[,1]=as.character(data[,1])\n\n    #Convert all columns except date column into numeric\n    for(j in 2:ncol(data)) data[,j]=as.numeric(as.character(data[,j]))\n\n    #FORMAT DATE\n    len=nchar(data[1,1])\n\n    #Monthly data\n    if(len==7) {\n      data[,1]=as.Date(paste(data[,1], \"-1\", sep=\"\"), \"%Y-%m-%d\")\n      data[,1]=sapply(data[,1], seq, length=2, by=\"1 month\")[2,]-1\n      data[,1]=as.Date(data[,1], \"%Y-%m-%d\", origin=\"1970-01-01\")\n    }\n\n    #Weekly data\n    if(len==23){\n      data[,1]=sapply(data[,1], substr, start=14, stop=30)\n      data[,1]=as.Date(data[,1], \"%Y-%m-%d\")\n    }\n\n    #Daily data\n    if(len==10) data[,1]=as.Date(data[,1], \"%Y-%m-%d\")\n\n    #Structure into panel data format\n    panelData=data[1:2]\n    panelData[3]=name[1]\n    names(panelData)=c(\"Date\", \"SVI\", \"Keyword\")\n    if(ncol(data)>2) {\n\n      for(j in 3:ncol(data)) {\n        appendData=data[c(1,j)]\n        appendData[3]=name[j-1]\n        names(appendData)=c(\"Date\", \"SVI\", \"Keyword\")\n        panelData=rbind(panelData, appendData)\n      }\n    }\n\n    #Add file name\n    panelData[ncol(panelData)+1]=filePath[i]\n\n    #Add path to filename\n    names(panelData)[4]=\"Path\"\n\n    #Merge several several files into one\n    if(i==1) output=panelData\n    if(i>1) output=rbind(output, panelData)\n  }\n  return(output)\n}\n\n# readAdditionalGT also takes the file path from the downloadGT function, but instead returns additional information from the file.\nreadAdditionalGT = function(filePath){\n  output=list()\n  rawFiles=list()\n  for(i in 1:length(filePath)){\n    if(length(filePath)==1) rawFiles[[1]]=read.csv(filePath, header=F, blank.lines.skip=F)\n    if(length(filePath)>1) rawFiles[[i]]=read.csv(filePath[i], header=F, blank.lines.skip=F)\n  }\n\n\n  for(file in rawFiles){\n    search_term = substring(as.character(file[1,1]), 22)\n    start = grep('Top regions', file[,1]) + 2\n    if(length(start)>0){\n      end = start + which(file[start:nrow(file),1]==\"\")[1] - 2\n      tmp = file[start:end,]\n      tmp$Type = 'Top regions'\n      tmp$Keyword = search_term\n      output[[length(output)+1]] = tmp\n    }\n\n    start = grep('Top cities', file[,1]) + 2\n    if(length(start)>0){\n      end = start + which(file[start:nrow(file),1]==\"\")[1] - 2\n      tmp = file[start:end,]\n      tmp$Type = 'Top cities'\n      tmp$Keyword = search_term\n      output[[length(output)+1]] = tmp\n    }\n\n    start = grep('Top searches', file[,1]) + 2\n    if(length(start)>0){\n      end = start + which(file[start:nrow(file),1]==\"\")[1] - 2\n      tmp = file[start:end,]\n      tmp$Type = 'Top searches'\n      tmp$Keyword = search_term\n      output[[length(output)+1]] = tmp\n    }\n\n    start = grep('Rising searches', file[,1]) + 2\n    if(length(start)>0){\n      end = start + which(file[start:nrow(file),1]==\"\")[1] - 2\n      tmp = file[start:end,]\n      tmp$Type = 'Rising searches'\n      tmp$Keyword = search_term\n      tmp$V2 = length(tmp$V2):1\n      output[[length(output)+1]] = tmp\n    }\n  }\n  output = do.call(rbind, output)\n  return(output)\n}\n",
    "created" : 1475156782689.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "4|79|34|0|\n37|38|50|0|\n53|26|126|0|\n129|38|179|0|\n",
    "hash" : "3745843747",
    "id" : "8B919785",
    "lastKnownWriteTime" : 1475157232,
    "path" : "~/GoogleTRends/R/GoogleTRends.R",
    "project_path" : "R/GoogleTRends.R",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "type" : "r_source"
}